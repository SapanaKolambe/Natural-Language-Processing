{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbIxDK4o4ot3KEGlQbApcq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SapanaKolambe/Natural-Language-Processing/blob/main/1_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenization"
      ],
      "metadata": {
        "id": "HBaPsPQaJtr-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfG4VgkyFwiQ",
        "outputId": "58db98df-5ff5-42d5-ac30-702828e1a9ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "# Split text by whitespace\n",
        "tokens = text.split()\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenization\n",
        "\n",
        "**Split()**\n",
        "\n",
        "\n",
        "split() method is used to break the given string in a sentence and return a list of strings by the specified separator."
      ],
      "metadata": {
        "id": "64fGs0K1JpBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets split the given text by full stop (.)\n",
        "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
        "text.split(\". \") # Note the space after the full stop makes sure that we dont get empty element at the end of list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKhV9bbOJlxg",
        "outputId": "207c0684-d8f9-403c-b064-ca7f4d4529ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
              " 'But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Using NLTK **\n",
        "\n",
        "Natural Language Toolkit (NLTK) is library written in python for natural language processing.\n",
        "\n",
        "NLTK has module word_tokenize() for word tokenization and sent_tokenize() for sentence tokenization.\n",
        "\n",
        "Syntax to install NLTK is as below:\n",
        "\n",
        "!pip install --user -U nltk\n",
        "\n",
        "Note that we are going use \"!\" before the command to let notebook know that, it should read as commandline command"
      ],
      "metadata": {
        "id": "TVHw61-SKTKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFG17bj8KOgu",
        "outputId": "6c4774d9-669e-4d31-8395-5f5e218f6373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFp5faqtKmfM",
        "outputId": "0b8ca44d-3e5b-4b55-f40f-baf602568e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmGW5Vs8K_Xc",
        "outputId": "f4c5b873-1cb4-4a1b-aa8b-ed69debcfd8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK word tokenization also consider the punctuation as token. During text cleaning process we have to account for this.\n",
        "\n",
        "**Sentence Tokenization**"
      ],
      "metadata": {
        "id": "dU7d6AX3Ky7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCQi7GCOKyu2",
        "outputId": "bf973307-0300-4015-9522-e1830fb7614e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
              " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
              " 'So sentence tonenization wont be foolproof with split() method.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regular Expression**\n",
        "\n",
        "Regex function is used to match or find strings using a sequence of patterns consisting of letters and numbers.\n",
        "\n",
        " We will re library to tokenize words and sentences of a paragraph."
      ],
      "metadata": {
        "id": "8lj-8FkGNlRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer \n",
        "tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
        "text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfci86gCMmyc",
        "outputId": "748ac471-edc8-4ed2-df87-cd28f048a922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Machine',\n",
              " 'learning',\n",
              " 'is',\n",
              " 'a',\n",
              " 'method',\n",
              " 'of',\n",
              " 'data',\n",
              " 'analysis',\n",
              " 'that',\n",
              " 'automates',\n",
              " 'analytical',\n",
              " 'model',\n",
              " 'building',\n",
              " 'It',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'that',\n",
              " 'systems',\n",
              " 'can',\n",
              " 'learn',\n",
              " 'from',\n",
              " 'data',\n",
              " 'identify',\n",
              " 'patterns',\n",
              " 'and',\n",
              " 'make',\n",
              " 'decisions',\n",
              " 'with',\n",
              " 'minimal',\n",
              " 'human',\n",
              " 'intervention']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Sentences**"
      ],
      "metadata": {
        "id": "Us94AMveN7kH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"\"\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\"\"\n",
        "sentences = re.compile('[.!?] ').split(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9clVR59N4iJ",
        "outputId": "b7e9c950-f192-4a2f-ef6b-976afb094232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Machine learning is a method of data analysis that automates analytical model building', 'It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.']\n"
          ]
        }
      ]
    }
  ]
}